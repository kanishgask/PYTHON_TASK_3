Research Plan

Paragraph 1 – Approach to identifying & evaluating models
I will begin by surveying freely available open-source models that specialize in code understanding, such as StarCoder, CodeGen, or CodeT5. The selection process will be based on factors like license type, context window size, resource requirements (RAM/VRAM), and suitability for deployment in an educational setting. For each candidate, I will evaluate its ability to analyze student-written Python programs and generate prompts that target conceptual understanding. The evaluation will use a dataset of student submissions containing both correct and erroneous code. Two pipelines will be followed: (1) asking the model to generate Socratic or diagnostic prompts that encourage reasoning, and (2) asking the model to identify possible misconceptions without revealing the full solution. The outputs will then be compared to expert teacher prompts for alignment with learning objectives.

Paragraph 2 – How to test & validate applicability
Validation will be conducted using a mix of offline and live testing. Offline testing involves comparing the generated prompts against a gold-standard set created by instructors and measuring quality using semantic similarity, relevance, and coverage across Bloom’s taxonomy levels. Live testing would involve classroom-style experiments where one group receives AI-generated prompts while another receives static hints, with learning outcomes (quiz scores, code revisions, time-to-correct) used as evaluation criteria. Additionally, failure cases such as hallucinations or overly prescriptive hints will be documented, alongside performance metrics like latency and compute requirements. The final analysis will provide recommendations on whether the model is directly applicable or requires fine-tuning, prompt engineering, or retrieval augmentation for effective use in student competence analysis.
